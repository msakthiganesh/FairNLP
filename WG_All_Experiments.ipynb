{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from transformers import RobertaForMultipleChoice, RobertaTokenizer, RobertaConfig, RobertaForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler\n",
    "\n",
    "import lime\n",
    "import torch.nn.functional as F\n",
    "from lime.lime_text import LimeTextExplainer\n",
    "\n",
    "import datetime, time\n",
    "import random\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "import pickle\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentid</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technician.customer.1.male.txt</td>\n",
       "      <td>The technician told the customer that he could...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technician.customer.1.female.txt</td>\n",
       "      <td>The technician told the customer that she coul...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technician.customer.1.neutral.txt</td>\n",
       "      <td>The technician told the customer that they cou...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              sentid  \\\n",
       "0     technician.customer.1.male.txt   \n",
       "1   technician.customer.1.female.txt   \n",
       "2  technician.customer.1.neutral.txt   \n",
       "\n",
       "                                            sentence  \n",
       "0  The technician told the customer that he could...  \n",
       "1  The technician told the customer that she coul...  \n",
       "2  The technician told the customer that they cou...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"dataset/winogender/all_sentences.tsv\", delimiter=\"\\t\")\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_words= [\"he\",\"she\",\"they\",\"his\",\"her\",\"their\",\"him\",\"them\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d27e432c91d4418b9b6b3a029b0ba6aa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "gender_arr = []\n",
    "for _, row in tqdm(df.iterrows()):\n",
    "    _s = row[\"sentence\"].split(\" \")\n",
    "    for gw in gender_words:\n",
    "        if gw in _s:\n",
    "            gender_arr.append(gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gender_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technician.customer.1.male.txt</td>\n",
       "      <td>The technician told the customer that he could...</td>\n",
       "      <td>he</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technician.customer.1.female.txt</td>\n",
       "      <td>The technician told the customer that she coul...</td>\n",
       "      <td>she</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technician.customer.1.neutral.txt</td>\n",
       "      <td>The technician told the customer that they cou...</td>\n",
       "      <td>they</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              sentid  \\\n",
       "0     technician.customer.1.male.txt   \n",
       "1   technician.customer.1.female.txt   \n",
       "2  technician.customer.1.neutral.txt   \n",
       "\n",
       "                                            sentence gender_word  \n",
       "0  The technician told the customer that he could...          he  \n",
       "1  The technician told the customer that she coul...         she  \n",
       "2  The technician told the customer that they cou...        they  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"gender_word\"] = gender_arr\n",
    "\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "he       178\n",
       "she      178\n",
       "they     178\n",
       "her       62\n",
       "his       54\n",
       "their     54\n",
       "him        8\n",
       "them       8\n",
       "Name: gender_word, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.gender_word.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### DF_MASK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mask = df[df['gender_word'].isin([\"he\", \"she\"])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_masked_sentences= []\n",
    "\n",
    "for _, row in df_mask.iterrows():\n",
    "    _masked_sentences.append(row[\"sentence\"].replace(\" he\",  \" <MASK>\").replace(\" she\", \" <MASK>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sentences = list(_masked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(masked_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mask[\"masked_sentences\"] = masked_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mask.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = []\n",
    "for index, row in df_mask.iterrows():\n",
    "    if(row['gender_word'] == 'he'): label.append(1)\n",
    "    else: label.append(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mask[\"label\"] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mask = df_mask.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mask.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.out_proj.bias', 'classifier.dense.weight', 'classifier.out_proj.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "config = RobertaConfig.from_pretrained(\"roberta-base\", num_labels=2)\n",
    "model = RobertaForSequenceClassification.from_pretrained(\"roberta-base\", config=config)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config = RobertaConfig.from_pretrained(\"models/roberta-base-masked/\", num_labels=2)\n",
    "# model = RobertaForSequenceClassification.from_pretrained(\"models/roberta-base-masked/\", config=config)\n",
    "# tokenizer = RobertaTokenizer.from_pretrained(\"models/roberta-base-masked/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Approach 1 - Using sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_preds = []\n",
    "for _, row in df_mask.iterrows():\n",
    "    sent = row[\"sentence\"]\n",
    "    inputs = tokenizer(sent, return_tensors=\"pt\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "        \n",
    "    pred_class_id = logits.argmax(dim=1).item()\n",
    "    sentence_preds.append(pred_class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(sentence_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Approach 2 - Using Masked Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mask.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_predictions = []\n",
    "for _, row in df_mask.iterrows():\n",
    "    inputs = tokenizer(row[\"masked_sentences\"], row[\"gender_word\"], return_tensors=\"pt\")\n",
    "    with torch.no_grad():\n",
    "        logits = model(**inputs).logits\n",
    "    predicted_class_id = logits.argmax(dim=1).item()\n",
    "    masked_predictions.append(predicted_class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(masked_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(texts):\n",
    "  outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
    "  probas = F.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "  return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [\"he\", \"she\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "df_mask[\"masked_sentences\"][i], df_mask[\"gender_word\"][i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(df_mask[\"masked_sentences\"][i], predictor, num_features=2, num_samples=100)\n",
    "exp.show_in_notebook(text=(df_mask[\"masked_sentences\"][i], df_mask[\"gender_word\"][i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Approach 3 - Finetuning with Masked Gender Word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "masked_sentence = []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    masked_sentence.append(row.sentence.replace(\" \" + row.gender_word, \" <mask>\"))\n",
    "\n",
    "df[\"masked_sentence\"] = masked_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentid</th>\n",
       "      <th>sentence</th>\n",
       "      <th>gender_word</th>\n",
       "      <th>masked_sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>technician.customer.1.male.txt</td>\n",
       "      <td>The technician told the customer that he could...</td>\n",
       "      <td>he</td>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>technician.customer.1.female.txt</td>\n",
       "      <td>The technician told the customer that she coul...</td>\n",
       "      <td>she</td>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>technician.customer.1.neutral.txt</td>\n",
       "      <td>The technician told the customer that they cou...</td>\n",
       "      <td>they</td>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>technician.someone.1.male.txt</td>\n",
       "      <td>The technician told someone that he could pay ...</td>\n",
       "      <td>he</td>\n",
       "      <td>The technician told someone that &lt;mask&gt; could ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>technician.someone.1.female.txt</td>\n",
       "      <td>The technician told someone that she could pay...</td>\n",
       "      <td>she</td>\n",
       "      <td>The technician told someone that &lt;mask&gt; could ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              sentid  \\\n",
       "0     technician.customer.1.male.txt   \n",
       "1   technician.customer.1.female.txt   \n",
       "2  technician.customer.1.neutral.txt   \n",
       "3      technician.someone.1.male.txt   \n",
       "4    technician.someone.1.female.txt   \n",
       "\n",
       "                                            sentence gender_word  \\\n",
       "0  The technician told the customer that he could...          he   \n",
       "1  The technician told the customer that she coul...         she   \n",
       "2  The technician told the customer that they cou...        they   \n",
       "3  The technician told someone that he could pay ...          he   \n",
       "4  The technician told someone that she could pay...         she   \n",
       "\n",
       "                                     masked_sentence  \n",
       "0  The technician told the customer that <mask> c...  \n",
       "1  The technician told the customer that <mask> c...  \n",
       "2  The technician told the customer that <mask> c...  \n",
       "3  The technician told someone that <mask> could ...  \n",
       "4  The technician told someone that <mask> could ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(720, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['he', 'she', 'they', 'his', 'her', 'their', 'him', 'them'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.gender_word.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['he', 'she', 'they', 'his', 'her', 'their', 'him', 'them']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gender_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "_masked_sentence, _org_gender_word, _gender_word, _label = [], [], [], []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for gw in gender_words:\n",
    "        if row.gender_word == gw:\n",
    "            _label.append(True)\n",
    "        else:\n",
    "            _label.append(False)\n",
    "        _masked_sentence.append(row.masked_sentence)\n",
    "        _org_gender_word.append(row.gender_word)\n",
    "        _gender_word.append(gw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5760"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(_masked_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_label[700]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>masked_sentence</th>\n",
       "      <th>gender_word</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>he</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>she</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>they</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>his</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>her</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>their</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>him</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>them</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>he</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>she</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>they</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>his</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>her</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>their</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>him</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>them</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>he</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>she</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>they</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>The technician told the customer that &lt;mask&gt; c...</td>\n",
       "      <td>his</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      masked_sentence gender_word  label\n",
       "0   The technician told the customer that <mask> c...          he   True\n",
       "1   The technician told the customer that <mask> c...         she  False\n",
       "2   The technician told the customer that <mask> c...        they  False\n",
       "3   The technician told the customer that <mask> c...         his  False\n",
       "4   The technician told the customer that <mask> c...         her  False\n",
       "5   The technician told the customer that <mask> c...       their  False\n",
       "6   The technician told the customer that <mask> c...         him  False\n",
       "7   The technician told the customer that <mask> c...        them  False\n",
       "8   The technician told the customer that <mask> c...          he  False\n",
       "9   The technician told the customer that <mask> c...         she   True\n",
       "10  The technician told the customer that <mask> c...        they  False\n",
       "11  The technician told the customer that <mask> c...         his  False\n",
       "12  The technician told the customer that <mask> c...         her  False\n",
       "13  The technician told the customer that <mask> c...       their  False\n",
       "14  The technician told the customer that <mask> c...         him  False\n",
       "15  The technician told the customer that <mask> c...        them  False\n",
       "16  The technician told the customer that <mask> c...          he  False\n",
       "17  The technician told the customer that <mask> c...         she  False\n",
       "18  The technician told the customer that <mask> c...        they   True\n",
       "19  The technician told the customer that <mask> c...         his  False"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df = pd.DataFrame(zip(_masked_sentence, _gender_word, _label),\n",
    "                   columns=['masked_sentence', 'gender_word', 'label'])\n",
    "\n",
    "_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5760, 3)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2960, 3)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_df = _df.drop_duplicates()\n",
    "mask_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# masked_sentence, label, org_gender_word, gender_word = [], [], [], []\n",
    "\n",
    "# for i in range(len(_masked_sentence)):\n",
    "#     if _label[i] == True:\n",
    "#         masked_sentence.append(_masked_sentence[i])\n",
    "#         org_gender_word.append(_org_gender_word[i])\n",
    "#         gender_word.append(_gender_word[i])\n",
    "#         label.append(_label[i])\n",
    "        \n",
    "# for i in range(len(_masked_sentence)):\n",
    "#     if _label[i] == False and _org_gender_word[i] != _gender_word[i]:\n",
    "#         masked_sentence.append(_masked_sentence[i])\n",
    "#         org_gender_word.append(_org_gender_word[i])\n",
    "#         gender_word.append(_gender_word[i])\n",
    "#         label.append(_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(masked_sentence), len(label), len(gender_word), len(org_gender_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# _i = 700\n",
    "# masked_sentence[_i], org_gender_word[_i], gender_word[_i], label[_i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "she      462\n",
       "he       452\n",
       "they     430\n",
       "her      348\n",
       "his      340\n",
       "their    340\n",
       "him      294\n",
       "them     294\n",
       "Name: gender_word, dtype: int64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_df.gender_word.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False    2240\n",
       "True      720\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mask_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1776, 3) (711, 3) (473, 3)\n"
     ]
    }
   ],
   "source": [
    "train_df = mask_df.groupby(\"label\").sample(frac=0.6, random_state=2023)\n",
    "val_test_df = mask_df.loc[mask_df.index.difference(train_df.index)]\n",
    "\n",
    "val_df = val_test_df.groupby(\"label\").sample(frac=0.6, random_state=2023)\n",
    "test_df = val_test_df.loc[val_test_df.index.difference(val_df.index)]\n",
    "\n",
    "print(train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(sentences, gender_word, label, maxlen):\n",
    "\n",
    "  encoded_input_ids = []\n",
    "  encoded_input_attn_mask = []\n",
    "\n",
    "  for i in tqdm(range(0, len(sentences))):\n",
    "    encoded_input = tokenizer.encode_plus(sentences[i],\n",
    "                                          gender_word[i],\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=maxlen,\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors=\"pt\",\n",
    "                                        truncation=True,\n",
    "                                        padding=\"max_length\")\n",
    "    \n",
    "    encoded_input_ids.append(encoded_input.input_ids)\n",
    "    encoded_input_attn_mask.append(encoded_input.attention_mask)\n",
    "\n",
    "  encoded_input_ids = torch.cat(encoded_input_ids, dim=0)\n",
    "  encoded_input_attn_mask = torch.cat(encoded_input_attn_mask, dim=0)\n",
    "  label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "  return (encoded_input_ids, encoded_input_attn_mask, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df.masked_sentence.values\n",
    "val_sentences = val_df.masked_sentence.values\n",
    "test_sentences = test_df.masked_sentence.values\n",
    "\n",
    "train_gender_word = train_df.gender_word.values\n",
    "val_gender_word = val_df.gender_word.values\n",
    "test_gender_word = test_df.gender_word.values\n",
    "\n",
    "train_labels = train_df.label.values\n",
    "val_labels = val_df.label.values\n",
    "test_labels = test_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9164de9b6696451c8309f362ebd1ae69",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1776 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dfcd8ea6ecd4916afbf090f8130abb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/711 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dd8d06a815447b196fdafcf6b06a8f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/473 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_encoded_input_ids, train_encoded_input_attn_mask, train_label = encode_input(sentences = train_sentences, gender_word = train_gender_word, label = train_labels, maxlen = 64)\n",
    "val_encoded_input_ids, val_encoded_input_attn_mask, val_label = encode_input(sentences = val_sentences,  gender_word = val_gender_word, label = val_labels, maxlen = 64)\n",
    "test_encoded_input_ids, test_encoded_input_attn_mask, test_label = encode_input(sentences = test_sentences,  gender_word = test_gender_word, label = test_labels, maxlen = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: The officer arrested the protester even though <mask> was not on duty.\n",
      "Token IDs: tensor([    0,   133,  1036,  1128,     5, 26065,   190,   600, 50264,    21,\n",
      "           45,    15,  4053,     4,     2,     2, 35369,     2,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
      "            1,     1,     1,     1])\n",
      "Attention Mask: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Label: False\n",
      "Decoded Token IDs: <s>The officer arrested the protester even though<mask> was not on duty.</s></s>them</s><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad><pad>\n"
     ]
    }
   ],
   "source": [
    "print(f\"Original Sentence: {train_sentences[0]}\")\n",
    "print(f\"Token IDs: {train_encoded_input_ids[0]}\")\n",
    "print(f\"Attention Mask: {train_encoded_input_attn_mask[0]}\")\n",
    "print(f\"Label: {train_labels[0]}\")\n",
    "print(f'Decoded Token IDs: {tokenizer.decode(train_encoded_input_ids[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_encoded_input_ids, train_encoded_input_attn_mask, train_label)\n",
    "eval_ds = TensorDataset(val_encoded_input_ids, val_encoded_input_attn_mask, val_label)\n",
    "test_ds= TensorDataset(test_encoded_input_ids, test_encoded_input_attn_mask, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"eos_token\" : \"</s>>\",\n",
    "    \"batch_size\" : 8,\n",
    "    \"random_seed\" : 2023\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_ds,  \n",
    "            sampler = RandomSampler(train_ds), \n",
    "            batch_size = config[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            eval_ds, \n",
    "            sampler = RandomSampler(eval_ds), \n",
    "            batch_size = config[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_ds, \n",
    "            sampler = RandomSampler(test_ds), \n",
    "            batch_size = config[\"batch_size\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/sakthi/opt/anaconda3/envs/fnlp/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "EPS = 1e-8\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = LR, eps = EPS)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                           num_warmup_steps = 0, \n",
    "                                           num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(config[\"random_seed\"])\n",
    "np.random.seed(config[\"random_seed\"])\n",
    "torch.manual_seed(config[\"random_seed\"])\n",
    "torch.cuda.manual_seed_all(config[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 5 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 63\u001b[0m\n\u001b[1;32m     59\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     61\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), \u001b[38;5;241m1.0\u001b[39m)\n\u001b[0;32m---> 63\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;66;03m# Update the learning rate.\u001b[39;00m\n\u001b[1;32m     66\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fnlp/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:68\u001b[0m, in \u001b[0;36m_LRScheduler.__init__.<locals>.with_counter.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     66\u001b[0m instance\u001b[38;5;241m.\u001b[39m_step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     67\u001b[0m wrapped \u001b[38;5;241m=\u001b[39m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__get__\u001b[39m(instance, \u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m---> 68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fnlp/lib/python3.10/site-packages/torch/optim/optimizer.py:140\u001b[0m, in \u001b[0;36mOptimizer._hook_for_profile.<locals>.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m profile_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizer.step#\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.step\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(obj\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[1;32m    139\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mrecord_function(profile_name):\n\u001b[0;32m--> 140\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    141\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    142\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/fnlp/lib/python3.10/site-packages/transformers/optimization.py:368\u001b[0m, in \u001b[0;36mAdamW.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    366\u001b[0m     bias_correction1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    367\u001b[0m     bias_correction2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m \u001b[38;5;241m-\u001b[39m beta2 \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m state[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstep\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 368\u001b[0m     step_size \u001b[38;5;241m=\u001b[39m step_size \u001b[38;5;241m*\u001b[39m \u001b[43mmath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbias_correction2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m bias_correction1\n\u001b[1;32m    370\u001b[0m p\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39maddcdiv_(exp_avg, denom, value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39mstep_size)\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# Just adding the square of the weights to the loss function is *not*\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# the correct way of using L2 regularization/weight decay with Adam,\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# since that will interact with the m and v parameters in strange ways.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;66;03m# of the weights to the loss with plain (non-momentum) SGD.\u001b[39;00m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;66;03m# Add weight decay at the end (fixed version)\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "training_stats = []\n",
    "epochs = EPOCHS\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('precision', 2)\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "# print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in tqdm(test_dataloader):\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  pred_labels = np.argmax(logits, axis=1)\n",
    "  label_ids = b_labels.numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.extend(pred_labels.tolist())\n",
    "  true_labels.extend(label_ids.tolist())\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_report= classification_report(true_labels, predictions, digits=3, output_dict=True)\n",
    "result_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/roberta-base-masked/\")\n",
    "tokenizer.save_pretrained(\"models/roberta-base-masked/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def predictor1(texts):\n",
    "    outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
    "    _tok_op = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "    # print(_tok_op)\n",
    "    print(tokenizer.convert_ids_to_tokens(_tok_op.input_ids[0]))\n",
    "    probas = F.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "class_names = [True, False]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "test_df = test_df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_str_to_predict, gender_word, gold_label = train_df.masked_sentence[1], train_df.gender_word[1], train_df.label[1]\n",
    "_str_to_predict, gender_word, gold_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(str_to_predict + '</s></s>' + gender_word + '</s>', predictor1, num_features=len(str_to_predict.split(\" \")), num_samples=10)\n",
    "# exp = explainer.explain_instance(str_to_predict + '</s></s>' + gender_word + '</s>', predictor1, num_features=3, num_samples=1000)\n",
    "exp.show_in_notebook(text=str_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "_op = tokenizer.encode_plus(_str_to_predict.replace(\"<MASK>\", \"<mask>\"), gender_word, max_length=64, return_tensors=\"pt\", padding=\"max_length\")\n",
    "_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "tokenizer.convert_ids_to_tokens(_op.input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def predictor2(texts):\n",
    "    outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
    "    _tok_op = tokenizer(texts, return_tensors=\"pt\", padding=True)\n",
    "    print(_tok_op)\n",
    "    probas = F.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "str_to_predict = _str_to_predict + '|' + gender_word\n",
    "# exp = explainer.explain_instance(str_to_predict, predictor, num_features=len(str_to_predict.split(\" \")), num_samples=100)\n",
    "exp = explainer.explain_instance(str_to_predict, predictor2, num_features=5, num_samples=100)\n",
    "exp.show_in_notebook(text=str_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def predictor(texts):\n",
    "  outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
    "  print(tokenizer.encode(texts, return_tensors=\"pt\", padding=True))\n",
    "  probas = F.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "  return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_str_to_predict, gender_word, gold_label = test_df.masked_sentence[0], test_df.gender_word[0], test_df.label[0]\n",
    "_str_to_predict, gender_word, gold_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_inp = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(\n",
    "    tokenizer.encode_plus(\n",
    "    _str_to_predict,\n",
    "    gender_word,\n",
    "    return_tensors='pt',\n",
    "    padding=True\n",
    ").input_ids[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.encode_plus(str_to_predict + '</s></s>' + gender_word + '</s>', return_tensors=\"pt\", padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_predict + '</s></s>' + gender_word + '</s>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(texts):\n",
    "    # print(texts)\n",
    "    sent, gender_word = texts[0].split(\"|\")[0], texts[0].split(\"|\")[1]\n",
    "    print(sent, gender_word)\n",
    "    _model_input = tokenizer(sent, gender_word, return_tensors=\"pt\", padding=True)\n",
    "    print(_model_input)\n",
    "    outputs = model()\n",
    "    print(tokenizer.encode(texts, return_tensors=\"pt\", padding=True))\n",
    "    probas = F.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "    return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_predict = _str_to_predict + '|' + gender_word\n",
    "# exp = explainer.explain_instance(str_to_predict, predictor, num_features=len(str_to_predict.split(\" \")), num_samples=100)\n",
    "exp = explainer.explain_instance(str_to_predict, predictor, num_features=5, num_samples=100)\n",
    "exp.show_in_notebook(text=str_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### Approach 4 - Finetuning with Gender Word Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_words= [\"he\",\"she\",\"they\",\"his\",\"her\",\"their\",\"him\",\"them\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_sentence, _label, _gender_word = [], [], []\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    for gw in gender_words:\n",
    "        if row.gender_word == gw:\n",
    "            _label.append(True)\n",
    "        else:\n",
    "            _label.append(False)\n",
    "        \n",
    "        _sentence.append(row.sentence.replace(\" \"+row.gender_word, \" \"+gw))\n",
    "        _gender_word.append(row.gender_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence, label, gender_word = [], [], []\n",
    "\n",
    "for i in range(len(_sentence)):\n",
    "    if _label[i] == True:\n",
    "        sentence.append(_sentence[i])\n",
    "        gender_word.append(_gender_word[i])\n",
    "        label.append(_label[i])\n",
    "        \n",
    "for i in range(len(_sentence)):\n",
    "    if _label[i] == False and _sentence[i] not in sentence:\n",
    "        sentence.append(_sentence[i])\n",
    "        gender_word.append(_gender_word[i])\n",
    "        label.append(_label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sentence), len(label), len(gender_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_df = pd.DataFrame(zip(sentence, gender_word, label), columns=[\"sentence\", \"gender_word\", \"label\"])\n",
    "extract_df = extract_df.sample(frac=1, random_state=2023).reset_index(drop=True)\n",
    "extract_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_df.sentence[0], extract_df.gender_word[0], extract_df.label[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = extract_df.groupby(\"label\").sample(frac=0.6, random_state=2023)\n",
    "val_test_df = extract_df.loc[extract_df.index.difference(train_df.index)]\n",
    "\n",
    "val_df = val_test_df.groupby(\"label\").sample(frac=0.6, random_state=2023)\n",
    "test_df = val_test_df.loc[val_test_df.index.difference(val_df.index)]\n",
    "\n",
    "print(train_df.shape, val_df.shape, test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_input(sentences, gender_word, label, maxlen):\n",
    "\n",
    "  encoded_input_ids = []\n",
    "  encoded_input_attn_mask = []\n",
    "\n",
    "  for i in tqdm(range(0, len(sentences))):\n",
    "    encoded_input = tokenizer.encode_plus(sentences[i],\n",
    "                                          gender_word[i],\n",
    "                                        add_special_tokens=True,\n",
    "                                        max_length=maxlen,\n",
    "                                        return_attention_mask=True,\n",
    "                                        return_tensors=\"pt\",\n",
    "                                        truncation=True,\n",
    "                                        padding=\"max_length\")\n",
    "    \n",
    "    encoded_input_ids.append(encoded_input.input_ids)\n",
    "    encoded_input_attn_mask.append(encoded_input.attention_mask)\n",
    "\n",
    "  encoded_input_ids = torch.cat(encoded_input_ids, dim=0)\n",
    "  encoded_input_attn_mask = torch.cat(encoded_input_attn_mask, dim=0)\n",
    "  label = torch.tensor(label, dtype=torch.long)\n",
    "\n",
    "  return (encoded_input_ids, encoded_input_attn_mask, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = train_df.sentence.values\n",
    "val_sentences = val_df.sentence.values\n",
    "test_sentences = test_df.sentence.values\n",
    "\n",
    "train_gender_word = train_df.gender_word.values\n",
    "val_gender_word = val_df.gender_word.values\n",
    "test_gender_word = test_df.gender_word.values\n",
    "\n",
    "train_labels = train_df.label.values\n",
    "val_labels = val_df.label.values\n",
    "test_labels = test_df.label.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_encoded_input_ids, train_encoded_input_attn_mask, train_label = encode_input(sentences = train_sentences, gender_word = train_gender_word, label = train_labels, maxlen = 64)\n",
    "val_encoded_input_ids, val_encoded_input_attn_mask, val_label = encode_input(sentences = val_sentences,  gender_word = val_gender_word, label = val_labels, maxlen = 64)\n",
    "test_encoded_input_ids, test_encoded_input_attn_mask, test_label = encode_input(sentences = test_sentences,  gender_word = test_gender_word, label = test_labels, maxlen = 64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Original Sentence: {train_sentences[0]}\")\n",
    "print(f\"Token IDs: {train_encoded_input_ids[0]}\")\n",
    "print(f\"Attention Mask: {train_encoded_input_attn_mask[0]}\")\n",
    "print(f\"Label: {train_labels[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(train_encoded_input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.decode(tokenizer(train_sentences[0], train_gender_word[0]).input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = TensorDataset(train_encoded_input_ids, train_encoded_input_attn_mask, train_label)\n",
    "eval_ds = TensorDataset(val_encoded_input_ids, val_encoded_input_attn_mask, val_label)\n",
    "test_ds= TensorDataset(test_encoded_input_ids, test_encoded_input_attn_mask, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"eos_token\" : \"[SEP]\",\n",
    "    \"batch_size\" : 8,\n",
    "    \"random_seed\" : 2023\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "            train_ds,  \n",
    "            sampler = RandomSampler(train_ds), \n",
    "            batch_size = config[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "validation_dataloader = DataLoader(\n",
    "            eval_ds, \n",
    "            sampler = RandomSampler(eval_ds), \n",
    "            batch_size = config[\"batch_size\"]\n",
    "        )\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "            test_ds, \n",
    "            sampler = RandomSampler(test_ds), \n",
    "            batch_size = config[\"batch_size\"]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "LR = 2e-5\n",
    "EPS = 1e-8\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr = LR, eps = EPS)\n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "                                           num_warmup_steps = 0, \n",
    "                                           num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "\n",
    "\n",
    "def format_time(elapsed):\n",
    "    '''\n",
    "    Takes a time in seconds and returns a string hh:mm:ss\n",
    "    '''\n",
    "    # Round to the nearest second.\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    \n",
    "    # Format as hh:mm:ss\n",
    "    return str(datetime.timedelta(seconds=elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(config[\"random_seed\"])\n",
    "np.random.seed(config[\"random_seed\"])\n",
    "torch.manual_seed(config[\"random_seed\"])\n",
    "torch.cuda.manual_seed_all(config[\"random_seed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_stats = []\n",
    "epochs = EPOCHS\n",
    "\n",
    "# Measure the total training time for the whole run.\n",
    "total_t0 = time.time()\n",
    "\n",
    "# For each epoch...\n",
    "for epoch_i in range(0, epochs):\n",
    "    \n",
    "    # ========================================\n",
    "    #               Training\n",
    "    # ========================================\n",
    "    \n",
    "    # Perform one full pass over the training set.\n",
    "\n",
    "    print(\"\")\n",
    "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "    print('Training...')\n",
    "\n",
    "    # Measure how long the training epoch takes.\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Reset the total loss for this epoch.\n",
    "    total_train_loss = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    # For each batch of training data...\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "\n",
    "        # Progress update every 40 batches.\n",
    "        if step % 40 == 0 and not step == 0:\n",
    "            # Calculate elapsed time in minutes.\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "\n",
    "\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "\n",
    "        model.zero_grad()        \n",
    "\n",
    "\n",
    "        result = model(b_input_ids, \n",
    "                       token_type_ids=None, \n",
    "                       attention_mask=b_input_mask, \n",
    "                       labels=b_labels,\n",
    "                       return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "\n",
    "        # Perform a backward pass to calculate the gradients.\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the learning rate.\n",
    "        scheduler.step()\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
    "    \n",
    "    # Measure how long this epoch took.\n",
    "    training_time = format_time(time.time() - t0)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "    print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "        \n",
    "    # ========================================\n",
    "    #               Validation\n",
    "    # ========================================\n",
    "    # After the completion of each training epoch, measure our performance on\n",
    "    # our validation set.\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"Running Validation...\")\n",
    "\n",
    "    t0 = time.time()\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    # Tracking variables \n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "    nb_eval_steps = 0\n",
    "\n",
    "    # Evaluate data for one epoch\n",
    "    for batch in validation_dataloader:\n",
    "\n",
    "        b_input_ids = batch[0]\n",
    "        b_input_mask = batch[1]\n",
    "        b_labels = batch[2]\n",
    "\n",
    "        with torch.no_grad():        \n",
    "\n",
    "\n",
    "            result = model(b_input_ids, \n",
    "                           token_type_ids=None, \n",
    "                           attention_mask=b_input_mask,\n",
    "                           labels=b_labels,\n",
    "                           return_dict=True)\n",
    "\n",
    "        loss = result.loss\n",
    "        logits = result.logits\n",
    "            \n",
    "        # Accumulate the validation loss.\n",
    "        total_eval_loss += loss.item()\n",
    "\n",
    "        # Move logits and labels to CPU\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
    "        \n",
    "\n",
    "    # Report the final accuracy for this validation run.\n",
    "    avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
    "    print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
    "\n",
    "    # Calculate the average loss over all of the batches.\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "    \n",
    "    # Measure how long the validation run took.\n",
    "    validation_time = format_time(time.time() - t0)\n",
    "    \n",
    "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
    "    print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "    # Record all statistics from this epoch.\n",
    "    training_stats.append(\n",
    "        {\n",
    "            'epoch': epoch_i + 1,\n",
    "            'Training Loss': avg_train_loss,\n",
    "            'Valid. Loss': avg_val_loss,\n",
    "            'Valid. Accur.': avg_val_accuracy,\n",
    "            'Training Time': training_time,\n",
    "            'Validation Time': validation_time\n",
    "        }\n",
    "    )\n",
    "\n",
    "print(\"\")\n",
    "print(\"Training complete!\")\n",
    "\n",
    "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.set_option('precision', 2)\n",
    "df_stats = pd.DataFrame(data=training_stats)\n",
    "df_stats = df_stats.set_index('epoch')\n",
    "\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use plot styling from seaborn.\n",
    "sns.set(style='darkgrid')\n",
    "\n",
    "# Increase the plot size and font size.\n",
    "sns.set(font_scale=1.5)\n",
    "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
    "\n",
    "# Plot the learning curve.\n",
    "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
    "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
    "\n",
    "# Label the plot.\n",
    "plt.title(\"Training & Validation Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.xticks([1, 2, 3, 4, 5, 6, 7])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "\n",
    "# print('Predicting labels for {:,} test sentences...'.format(len(input_ids)))\n",
    "\n",
    "# Put model in evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Tracking variables \n",
    "predictions , true_labels = [], []\n",
    "\n",
    "# Predict \n",
    "for batch in tqdm(test_dataloader):\n",
    "  # Add batch to GPU\n",
    "  batch = tuple(t for t in batch)\n",
    "  \n",
    "  # Unpack the inputs from our dataloader\n",
    "  b_input_ids, b_input_mask, b_labels = batch\n",
    "  \n",
    "  # Telling the model not to compute or store gradients, saving memory and \n",
    "  # speeding up prediction\n",
    "  with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions.\n",
    "      result = model(b_input_ids, \n",
    "                     token_type_ids=None, \n",
    "                     attention_mask=b_input_mask,\n",
    "                     return_dict=True)\n",
    "\n",
    "  logits = result.logits\n",
    "\n",
    "  # Move logits and labels to CPU\n",
    "  logits = logits.detach().cpu().numpy()\n",
    "  pred_labels = np.argmax(logits, axis=1)\n",
    "  label_ids = b_labels.numpy()\n",
    "  \n",
    "  # Store predictions and true labels\n",
    "  predictions.extend(pred_labels.tolist())\n",
    "  true_labels.extend(label_ids.tolist())\n",
    "\n",
    "print('DONE.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_report= classification_report(true_labels, predictions, digits=3, output_dict=True)\n",
    "result_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(true_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"models/roberta-base/\")\n",
    "tokenizer.save_pretrained(\"models/roberta-base/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### XAI for SC on Finetuned RoBERTa-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(texts):\n",
    "  outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
    "  print(tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
    "  probas = F.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "  return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_predict, gender_word, gold_label = test_df.sentence[0], test_df.gender_word[0], test_df.label[0]\n",
    "str_to_predict, gender_word, gold_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def text_tokenizer(sentence, gender_word):\n",
    "#     return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(str_to_predict + '</s></s>' + gender_word + '</s>', predictor, num_features=len(str_to_predict.split(\" \")), num_samples=100)\n",
    "# exp = explainer.explain_instance(str_to_predict, predictor, num_features=5, num_samples=100)\n",
    "exp.show_in_notebook(text=str_to_predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "#### XAI on DFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(texts):\n",
    "  outputs = model(**tokenizer(texts, return_tensors=\"pt\", padding=True))\n",
    "  probas = F.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "  return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = [False, True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explain, val_explain, test_explain = [], [], []\n",
    "\n",
    "for i in tqdm(range(len(train_sentences))): \n",
    "  sent = train_sentences[i] + '</s></s>' + train_gender_word[i] + '</s>'\n",
    "  length = len(sent.split(\" \"))\n",
    "  exp = explainer.explain_instance(sent, predictor, num_features=len(train_sentences[i].split(\" \")), num_samples = 100)\n",
    "  train_explain.append(exp.as_list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('explain/train_explain', 'wb') as fp:\n",
    "    pickle.dump(train_explain, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(val_sentences))): \n",
    "  sent = val_sentences[i] + '</s></s>' + val_gender_word[i] + '</s>'\n",
    "  length = len(sent.split(\" \"))\n",
    "  exp = explainer.explain_instance(sent, predictor, num_features=len(val_sentences[i].split(\" \")), num_samples = 100)\n",
    "  val_explain.append(exp.as_list())\n",
    "  \n",
    "with open('explain/val_explain', 'wb') as fp:\n",
    "    pickle.dump(val_explain, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in tqdm(range(len(test_sentences))): \n",
    "  sent = test_sentences[i] + '</s></s>' + test_gender_word[i] + '</s>'\n",
    "  length = len(sent.split(\" \"))\n",
    "  exp = explainer.explain_instance(sent, predictor, num_features=len(test_sentences[i].split(\" \")), num_samples = 100)\n",
    "  test_explain.append(exp.as_list())\n",
    "  \n",
    "with open('explain/test_explain', 'wb') as fp:\n",
    "    pickle.dump(test_explain, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explain[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('explain/train_explain', 'rb') as fp:\n",
    "    train_explain = pickle.load(fp)\n",
    "\n",
    "with open ('explain/val_explain', 'rb') as fp:\n",
    "    val_explain = pickle.load(fp)\n",
    "\n",
    "with open ('explain/test_explain', 'rb') as fp:\n",
    "    test_explain = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### MC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RobertaTokenizer.from_pretrained(\"roberta-base\")\n",
    "model = RobertaForMultipleChoice.from_pretrained(\"roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "choices = gender_words\n",
    "preds =[]\n",
    "\n",
    "for _, row in df.iterrows():\n",
    "    prompt = row.sentence.replace(\" he\", \" <MASK>\")\n",
    "    encoding = tokenizer([prompt] * len(choices), choices, return_tensors=\"pt\", padding=True)\n",
    "    outputs = model(**{k:v.unsqueeze(0) for k, v in encoding.items()})\n",
    "    pred = outputs.logits.argmax(dim=1)\n",
    "    # print(F.softmax(outputs.logits, dim=1).detach().numpy())\n",
    "    # print(prompt, choices[pred])\n",
    "    preds.append(choices[pred])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"preds\"] = preds\n",
    "\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.preds.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.gender_word.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### XAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lime\n",
    "import torch.nn.functional as F\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictor(texts):\n",
    "  encoding = tokenizer([texts] * len(choices), choices, return_tensors=\"pt\", padding=True)\n",
    "  outputs = model(**{k:v.unsqueeze(0) for k, v in encoding.items()})\n",
    "  probas = F.softmax(outputs.logits, dim=1).detach().numpy()\n",
    "  print(probas)\n",
    "  return probas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gender_words= [\"he\",\"she\",\"they\",\"his\",\"her\",\"their\",\"him\",\"them\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer = LimeTextExplainer(class_names=gender_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "str_to_predict =  df.sentence[0].replace(\" he\", \" <MASK>\")\n",
    "str_to_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp = explainer.explain_instance(str_to_predict, predictor, num_features=2)\n",
    "exp.show_in_notebook(text=str_to_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "50b845e7524912447941e04259c8a1ee078d93d55b2a8ea43622acffd569f09d"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
